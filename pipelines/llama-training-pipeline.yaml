###############################################################################
# LLaMA Training Pipeline (Kubeflow/Argo Workflow)
# DAG: λ°μ΄ν„°μ „μ²λ¦¬ β†’ λ¨λΈν•™μµ β†’ λ¨λΈν‰κ°€
# AI Storage Scheduler μ‚¬μ©
###############################################################################
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: llama-training-pipeline
  namespace: kubeflow-user-example-com
  labels:
    app: llama-pipeline
    workload-type: text
    framework: pytorch
spec:
  entrypoint: llama-pipeline
  serviceAccountName: default-editor

  # κ³µμ  λ³Όλ¥¨ (λ‹¨κ³„ κ°„ λ°μ΄ν„° μ „λ‹¬)
  volumeClaimTemplates:
  - metadata:
      name: pipeline-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 5Gi

  # DAG μ •μ
  templates:
  #===========================================================================
  # Main DAG Pipeline
  #===========================================================================
  - name: llama-pipeline
    dag:
      tasks:
      # Step 1: λ°μ΄ν„° μ „μ²λ¦¬
      - name: preprocess
        template: preprocess-step

      # Step 2: λ¨λΈ ν•™μµ (μ „μ²λ¦¬ μ™„λ£ ν›„)
      - name: train
        template: train-step
        dependencies: [preprocess]

      # Step 3: λ¨λΈ ν‰κ°€ (ν•™μµ μ™„λ£ ν›„)
      - name: evaluate
        template: evaluate-step
        dependencies: [train]

  #===========================================================================
  # Step 1: λ°μ΄ν„° μ „μ²λ¦¬ (AI Storage Scheduler)
  #===========================================================================
  - name: preprocess-step
    metadata:
      labels:
        pipeline-step: preprocess
        workload-type: text
      annotations:
        ai-storage.keti/preprocessing-type: "transformation"
        ai-storage.keti/io-pattern: "write-heavy"
        ai-storage.keti/workload-stage: "preprocessing"
    # AI Storage Scheduler μ‚¬μ©
    schedulerName: ai-storage-scheduler
    tolerations:
    - operator: Exists
    container:
      image: python:3.11-slim
      command: [python3, -c]
      args:
      - |
        import time
        import sys
        import os

        sys.argv[0] = 'llama_data_preprocessor.py'
        node = os.environ.get('NODE_NAME', 'unknown')

        print("=" * 60)
        print("[STEP 1/3] Data Preprocessing")
        print(f"Scheduled Node: {node}")
        print("=" * 60)
        print("Task: Tokenization & Data Preparation for LLaMA")

        os.makedirs('/data/processed', exist_ok=True)
        os.makedirs('/data/tokens', exist_ok=True)

        stages = ['load_dataset', 'tokenize', 'create_batches', 'save_processed']
        for i, stage in enumerate(stages):
            print(f"\n[{time.strftime('%H:%M:%S')}] Stage {i+1}/4: {stage}")
            with open(f'/data/processed/{stage}.bin', 'wb') as f:
                f.write(os.urandom(5 * 1024 * 1024))
            time.sleep(15)

        with open('/data/tokens/train_tokens.bin', 'wb') as f:
            f.write(os.urandom(20 * 1024 * 1024))

        print("\n" + "=" * 60)
        print("[STEP 1/3] Preprocessing COMPLETED")
        print(f"Node: {node}")
        print("=" * 60)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            fieldPath: spec.nodeName
      resources:
        requests:
          cpu: "200m"
          memory: "256Mi"
      volumeMounts:
      - name: pipeline-data
        mountPath: /data

  #===========================================================================
  # Step 2: λ¨λΈ ν•™μµ (AI Storage Scheduler)
  #===========================================================================
  - name: train-step
    metadata:
      labels:
        pipeline-step: train
        workload-type: text
      annotations:
        ai-storage.keti/preprocessing-type: "aggregation"
        ai-storage.keti/io-pattern: "balanced"
        ai-storage.keti/workload-stage: "training"
    schedulerName: ai-storage-scheduler
    tolerations:
    - operator: Exists
    container:
      image: python:3.11-slim
      command: [python3, -c]
      args:
      - |
        import time
        import sys
        import os
        import random

        sys.argv[0] = 'llama_pytorch_trainer.py'
        node = os.environ.get('NODE_NAME', 'unknown')

        print("=" * 60)
        print("[STEP 2/3] Model Training")
        print(f"Scheduled Node: {node}")
        print("=" * 60)
        print("Model: LLaMA-7B (Simulated)")
        print("Framework: PyTorch + HuggingFace")

        os.makedirs('/data/checkpoints', exist_ok=True)

        if os.path.exists('/data/tokens/train_tokens.bin'):
            print("β“ Preprocessed data found")
        else:
            print("β  Using simulated data")

        EPOCHS = 2
        STEPS_PER_EPOCH = 5

        for epoch in range(EPOCHS):
            print(f"\n[Epoch {epoch+1}/{EPOCHS}]")
            for step in range(STEPS_PER_EPOCH):
                loss = 2.0 - (epoch * 0.3) - (step * 0.02) + random.uniform(-0.05, 0.05)
                with open(f'/data/checkpoints/step_{epoch}_{step}.bin', 'wb') as f:
                    f.write(os.urandom(1024 * 1024))
                print(f"  Step {step+1}/{STEPS_PER_EPOCH}: loss={loss:.4f}")
                time.sleep(5)

            ckpt_path = f'/data/checkpoints/llama_epoch_{epoch+1}.pt'
            with open(ckpt_path, 'wb') as f:
                f.write(os.urandom(10 * 1024 * 1024))
            print(f"  Checkpoint: {ckpt_path}")

        with open('/data/checkpoints/llama_final.pt', 'wb') as f:
            f.write(os.urandom(20 * 1024 * 1024))

        print("\n" + "=" * 60)
        print("[STEP 2/3] Training COMPLETED")
        print(f"Node: {node}")
        print("=" * 60)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            fieldPath: spec.nodeName
      resources:
        requests:
          cpu: "500m"
          memory: "512Mi"
      volumeMounts:
      - name: pipeline-data
        mountPath: /data

  #===========================================================================
  # Step 3: λ¨λΈ ν‰κ°€ (AI Storage Scheduler)
  #===========================================================================
  - name: evaluate-step
    metadata:
      labels:
        pipeline-step: evaluate
        workload-type: text
      annotations:
        ai-storage.keti/preprocessing-type: "filtering"
        ai-storage.keti/io-pattern: "read-heavy"
        ai-storage.keti/workload-stage: "evaluation"
    schedulerName: ai-storage-scheduler
    tolerations:
    - operator: Exists
    container:
      image: python:3.11-slim
      command: [python3, -c]
      args:
      - |
        import time
        import sys
        import os
        import random
        import json

        sys.argv[0] = 'llama_model_evaluator.py'
        node = os.environ.get('NODE_NAME', 'unknown')

        print("=" * 60)
        print("[STEP 3/3] Model Evaluation")
        print(f"Scheduled Node: {node}")
        print("=" * 60)
        print("Task: Evaluate LLaMA model performance")

        os.makedirs('/data/results', exist_ok=True)

        model_path = '/data/checkpoints/llama_final.pt'
        if os.path.exists(model_path):
            print(f"β“ Model loaded: {model_path}")
        else:
            print("β  Using simulated model")

        metrics = {
            'perplexity': random.uniform(15, 25),
            'bleu_score': random.uniform(0.3, 0.5),
            'rouge_l': random.uniform(0.4, 0.6),
            'accuracy': random.uniform(0.75, 0.90)
        }

        print("\nRunning evaluation...")
        for i in range(5):
            print(f"  Evaluating batch {i+1}/5...")
            time.sleep(5)

        print("\n" + "=" * 60)
        print("Evaluation Results:")
        print("=" * 60)
        for metric, value in metrics.items():
            print(f"  {metric}: {value:.4f}")

        with open('/data/results/evaluation.json', 'w') as f:
            json.dump(metrics, f, indent=2)

        print("\n" + "=" * 60)
        print("[STEP 3/3] Evaluation COMPLETED")
        print(f"Node: {node}")
        print("=" * 60)
        print("\nπ‰ PIPELINE COMPLETED SUCCESSFULLY!")
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            fieldPath: spec.nodeName
      resources:
        requests:
          cpu: "200m"
          memory: "256Mi"
      volumeMounts:
      - name: pipeline-data
        mountPath: /data
